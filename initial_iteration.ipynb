{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install tokenizers\n"
      ],
      "metadata": {
        "id": "fzLiDxloFT2s"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install PyPDF2"
      ],
      "metadata": {
        "id": "2aDjJH9p3bey",
        "outputId": "81d26cf4-1938-4a85-e826-1b582a56d19a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "my_text = \"\"\"The Advertisement was telecasted nationwide, and the product was sold in around 30 states of America. The product became so successful among the people that the production was increased. Two new plant sites were finalized, and the construction was started. Now, The Cloud Enterprise became one of America's biggest firms and the mass producer in all major sectors, from transportation to personal care. Director of The Cloud Enterprise, Ryan Cloud, was now started getting interviewed over his success stories. Many popular magazines were started publishing Critiques about him.\"\"\"\n"
      ],
      "metadata": {
        "id": "aFugqm2EFo3c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "vRXc-bX9GKxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(my_text)\n"
      ],
      "metadata": {
        "id": "ODXPg_q-GEk5",
        "outputId": "fc9a889b-796e-412e-c716-bc6f7de75180",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Advertisement was telecasted nationwide, and the product was sold in around 30 states of America.',\n",
              " 'The product became so successful among the people that the production was increased.',\n",
              " 'Two new plant sites were finalized, and the construction was started.',\n",
              " \"Now, The Cloud Enterprise became one of America's biggest firms and the mass producer in all major sectors, from transportation to personal care.\",\n",
              " 'Director of The Cloud Enterprise, Ryan Cloud, was now started getting interviewed over his success stories.',\n",
              " 'Many popular magazines were started publishing Critiques about him.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# creating a pdf reader object\n",
        "reader = PdfReader('/content/1709.06680.pdf')\n",
        "\n",
        "# printing number of pages in pdf file\n",
        "print(len(reader.pages))\n",
        "\n",
        "# getting a specific page from the pdf file\n",
        "page = reader.pages[0]\n",
        "\n",
        "# extracting text from page\n",
        "text = page.extract_text()\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "kpUEE2SQ3ICS",
        "outputId": "e7105eda-4c90-4a81-ae97-bf33feaa5b5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "Deep Lattice Networks and Partial Monotonic\n",
            "Functions\n",
            "Seungil You, David Ding, Kevin Canini, Jan Pfeifer, Maya Gupta\n",
            "Google Inc.\n",
            "1600 Amphitheatre Parkway, Mountain View, CA 94043\n",
            "{siyou,dwding,canini,janpf,mayagupta}@google.com\n",
            "Abstract\n",
            "We propose learning deep models that are monotonic with respect to a user-\n",
            "speciﬁed set of inputs by alternating layers of linear embeddings, ensembles of\n",
            "lattices, and calibrators (piecewise linear functions), with appropriate constraints\n",
            "for monotonicity, and jointly training the resulting network. We implement the\n",
            "layers and projections with new computational graph nodes in TensorFlow and use\n",
            "the ADAM optimizer and batched stochastic gradients. Experiments on benchmark\n",
            "and real-world datasets show that six-layer monotonic deep lattice networks achieve\n",
            "state-of-the art performance for classiﬁcation and regression with monotonicity\n",
            "guarantees.\n",
            "1 Introduction\n",
            "We propose building models with multiple layers of lattices, which we refer to as deep lattice networks\n",
            "(DLNs). While we hypothesize that DLNs may generally be useful, we focus on the challenge of\n",
            "learning ﬂexible partially-monotonic functions, that is, models that are guaranteed monotonic with\n",
            "respect to a user-speciﬁed subset of the inputs. For example, if one is predicting whether to give\n",
            "someone else a loan, we expect and would like to constrain the prediction to be monotonically\n",
            "increasing with respect to the applicant’s income, if all other features are unchanged. Imposing\n",
            "monotonicity acts as a regularizer, improves generalization to test data, and makes the end-to-end\n",
            "model more interpretable, debuggable, and trustworthy.\n",
            "To learn more ﬂexible partial monotonic functions, we propose architectures that alternate three\n",
            "kinds of layers: linear embeddings, calibrators, and ensembles of lattices, each of which is trained\n",
            "discriminatively to optimize a structural risk objective and obey any given monotonicity constraints.\n",
            "See Fig. 2 for an example DLN with nine such layers.\n",
            "Lattices are interpolated look-up tables, as shown in Fig. 1. Lattices have been shown to be an\n",
            "efﬁcient nonlinear function class that can be constrained to be monotonic by adding appropriate sparse\n",
            "linear inequalities on the parameters [ 1], and can be trained in a standard empirical risk minimization\n",
            "framework [ 2,1]. Recent work showed lattices could be jointly trained as an ensemble to learn\n",
            "ﬂexible monotonic functions for an arbitrary number of inputs [3].\n",
            "Calibrators are one-dimensional lattices, which nonlinearly transform a single input [ 1]; see Fig. 1 for\n",
            "an example. They have been used to pre-process inputs in two-layer models: calibrators-then-linear\n",
            "models [ 4], calibrators-then-lattice models [ 1], and calibrators-then-ensemble-of-lattices model [ 3].\n",
            "Here, we extend their use to discriminatively normalize between other layers of the deep model, as\n",
            "well as act as a pre-processing layer. We also ﬁnd that using a calibrator for a last layer can help\n",
            "nonlinearly transform the outputs to better match the labels.\n",
            "We ﬁrst describe the proposed DLN layers in detail in Section 2. In Section 3, we review more related\n",
            "work in learning ﬂexible partial monotonic functions. We provide theoretical results characterizing\n",
            "the ﬂexibility of the DLN in Section 4, followed by details on our TensorFlow implementation and\n",
            "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1709.06680v1  [stat.ML]  19 Sep 2017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = text.replace(\"\\n\", \" \")"
      ],
      "metadata": {
        "id": "702ttCyh3Hzb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(new_text)"
      ],
      "metadata": {
        "id": "FvHW_6F44Ov6",
        "outputId": "32e8581e-44cc-48ca-d8dc-4eef98620cd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Deep Lattice Networks and Partial Monotonic Functions Seungil You, David Ding, Kevin Canini, Jan Pfeifer, Maya Gupta Google Inc. 1600 Amphitheatre Parkway, Mountain View, CA 94043 {siyou,dwding,canini,janpf,mayagupta}@google.com Abstract We propose learning deep models that are monotonic with respect to a user- speciﬁed set of inputs by alternating layers of linear embeddings, ensembles of lattices, and calibrators (piecewise linear functions), with appropriate constraints for monotonicity, and jointly training the resulting network.',\n",
              " 'We implement the layers and projections with new computational graph nodes in TensorFlow and use the ADAM optimizer and batched stochastic gradients.',\n",
              " 'Experiments on benchmark and real-world datasets show that six-layer monotonic deep lattice networks achieve state-of-the art performance for classiﬁcation and regression with monotonicity guarantees.',\n",
              " '1 Introduction We propose building models with multiple layers of lattices, which we refer to as deep lattice networks (DLNs).',\n",
              " 'While we hypothesize that DLNs may generally be useful, we focus on the challenge of learning ﬂexible partially-monotonic functions, that is, models that are guaranteed monotonic with respect to a user-speciﬁed subset of the inputs.',\n",
              " 'For example, if one is predicting whether to give someone else a loan, we expect and would like to constrain the prediction to be monotonically increasing with respect to the applicant’s income, if all other features are unchanged.',\n",
              " 'Imposing monotonicity acts as a regularizer, improves generalization to test data, and makes the end-to-end model more interpretable, debuggable, and trustworthy.',\n",
              " 'To learn more ﬂexible partial monotonic functions, we propose architectures that alternate three kinds of layers: linear embeddings, calibrators, and ensembles of lattices, each of which is trained discriminatively to optimize a structural risk objective and obey any given monotonicity constraints.',\n",
              " 'See Fig.',\n",
              " '2 for an example DLN with nine such layers.',\n",
              " 'Lattices are interpolated look-up tables, as shown in Fig.',\n",
              " '1.',\n",
              " 'Lattices have been shown to be an efﬁcient nonlinear function class that can be constrained to be monotonic by adding appropriate sparse linear inequalities on the parameters [ 1], and can be trained in a standard empirical risk minimization framework [ 2,1].',\n",
              " 'Recent work showed lattices could be jointly trained as an ensemble to learn ﬂexible monotonic functions for an arbitrary number of inputs [3].',\n",
              " 'Calibrators are one-dimensional lattices, which nonlinearly transform a single input [ 1]; see Fig.',\n",
              " '1 for an example.',\n",
              " 'They have been used to pre-process inputs in two-layer models: calibrators-then-linear models [ 4], calibrators-then-lattice models [ 1], and calibrators-then-ensemble-of-lattices model [ 3].',\n",
              " 'Here, we extend their use to discriminatively normalize between other layers of the deep model, as well as act as a pre-processing layer.',\n",
              " 'We also ﬁnd that using a calibrator for a last layer can help nonlinearly transform the outputs to better match the labels.',\n",
              " 'We ﬁrst describe the proposed DLN layers in detail in Section 2.',\n",
              " 'In Section 3, we review more related work in learning ﬂexible partial monotonic functions.',\n",
              " 'We provide theoretical results characterizing the ﬂexibility of the DLN in Section 4, followed by details on our TensorFlow implementation and 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1709.06680v1  [stat.ML]  19 Sep 2017']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TaI2JN7wGvT9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}